{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeating the principal component analysis but with different imputations for number of months food insecure\n",
    "# imputing using the means should be the purest method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I couldn't find a good package that did what I had expected it to do so I did some investigation and experimentation\n",
    "    # using a few. This was my favourite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import statsmodels.multivariate.pca as smp\n",
    "import statsmodels.multivariate.factor as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import eye, asarray, dot, sum, diag\n",
    "from numpy.linalg import svd\n",
    "# ... from wikipedia or something...\n",
    "def varimax(Phi, gamma = 1.0, q = 20, tol = 1e-6):\n",
    "    p,k = Phi.shape\n",
    "    R = eye(k)\n",
    "    d=0\n",
    "    for i in range(q):\n",
    "        d_old = d\n",
    "        Lambda = dot(Phi, R)\n",
    "        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, diag(diag(dot(Lambda.T,Lambda))))))\n",
    "        R = dot(u,vh)\n",
    "        d = sum(s)\n",
    "        if d_old!=0 and d/d_old < 1 + tol: break\n",
    "    return dot(Phi, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise(dataframe):\n",
    "    for i in dataframe.columns:\n",
    "        (m,s) = (dataframe[i].mean(), dataframe[i].std())\n",
    "        dataframe.loc[:,i]=dataframe.loc[:,i]-m\n",
    "        dataframe.loc[:,i]=np.divide(dataframe.loc[:,i],s)\n",
    "        print (\"for column \",str(i), \", the mean was \",m,\" and the standard deviation was \",s)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scree(fa):\n",
    "    eigs = fa.eigenvals\n",
    "    index = np.arange(len(eigs))+1\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.plot(index, eigs, 'bo-')\n",
    "    ax.set_title('Scree Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_loadings(data, min_facs=2, max_facs=3):\n",
    "    fig = plt.figure(figsize=(5,8))\n",
    "    axes = fig.subplots(max_facs-min_facs+1)\n",
    "    view=[]\n",
    "    model=[]\n",
    "    for i in range(max_facs-min_facs+1):\n",
    "        list_facs=['Factor 1']\n",
    "        for j in range(i+min_facs-1):\n",
    "            list_facs.append(str('Factor '+str(j+2)))\n",
    "        paf = smf.Factor(data, missing='drop', n_factor=i+min_facs).fit()\n",
    "        paf.rotate('oblimin')\n",
    "        view.append(pd.DataFrame(paf.loadings, index=data.columns, columns = list_facs[:i+min_facs]))\n",
    "        sns.heatmap(view[i], annot = True, ax=axes[i], cmap='PiYG')\n",
    "        model.append(paf)\n",
    "    return(view, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_score_matrix(data, min_facs=2, max_facs=3):\n",
    "    fig = plt.figure(figsize=(5,8))\n",
    "    axes = fig.subplots(max_facs-min_facs+1)\n",
    "    view=[]\n",
    "    model=[]\n",
    "    for i in range(max_facs-min_facs+1):\n",
    "        list_facs=['Factor 1']\n",
    "        for j in range(i+min_facs-1):\n",
    "            list_facs.append(str('Factor '+str(j+2)))\n",
    "        paf = smf.Factor(data, missing='drop', n_factor=i+min_facs).fit()\n",
    "        paf.rotate('oblimin')\n",
    "        paf_score=paf.factor_score_params('regression')\n",
    "        view.append(pd.DataFrame(paf_score, index=data.columns, columns = list_facs[:i+min_facs]))\n",
    "        sns.heatmap(view[i], annot = True, ax=axes[i], cmap='PiYG')\n",
    "        model.append(paf)\n",
    "    return(view, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factor_scores(data, loadings, names = None):\n",
    "    # give the columns titles\n",
    "    if names:\n",
    "        labs = names\n",
    "    else:\n",
    "        labs = []\n",
    "        for i in range(len(loadings.columns)):\n",
    "            labs.append(str(\"Factor \"+str(i+1)))\n",
    "    # make a dataframe for the scores\n",
    "    scores = (np.zeros((len(data),len(loadings.columns))))\n",
    "    scores = pd.DataFrame(scores, columns=labs)\n",
    "    # fill the dataframe\n",
    "    for i in range(len(scores.columns)):\n",
    "        for j in range(len(loadings)):\n",
    "            scores[scores.columns[i]] += loadings.iloc[j,i]*data.loc[:,data.columns[j]]\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators = pd.read_csv(\"2PCAOutput\",encoding = \"ISO-8859-1\")\n",
    "#indicators[3000:3010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators_success=indicators[['NrofMonthsFoodInsecure','score_PPI','HFIAS_code','score_HDDSBadSeason', 'score_HDDSGoodSeason',\n",
    "                               'Income/MAE decile', 'Income/MAE for country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the means to impute them\n",
    "\n",
    "means={}\n",
    "for i in indicators_success.columns:\n",
    "    means.setdefault(i,indicators_success[i].mean())\n",
    "indicators_mean_imputed=indicators_success.fillna(value=means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means imputed\n",
    "# scree plot - eigenvalues of correlation matrix\n",
    "p=np.linalg.eig(indicators_mean_imputed.corr())\n",
    "index = np.arange(len(p[0]))+1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(index, pd.Series(p[0]).sort_values(ascending=False), 'bo-')\n",
    "ax.set_title('Scree Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means imputed\n",
    "# scree plot of the eigenvalues extracted by the statsmodels PCA\n",
    "pca = smp.PCA(indicators_mean_imputed, method = 'nipals', ncomp=7)\n",
    "scree(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# means imputed\n",
    "# let's go for 2 and also for 3 factors!\n",
    "\n",
    "# Using PCA, then PAF below\n",
    "\n",
    "pca2 = smp.PCA(indicators_mean_imputed, method = 'nipals', ncomp=2)\n",
    "# sns.heatmap(pca2.loadings, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca3 = smp.PCA(indicators_mean_imputed, method = 'nipals', ncomp=3)\n",
    "# sns.heatmap(pca3.loadings, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA2rotated = varimax(pca2.loadings)\n",
    "pca2loadings = pd.DataFrame(PCA2rotated, index=indicators_mean_imputed.columns, columns = ['Factor 1', 'Factor 2'])\n",
    "sns.heatmap(pca2loadings, annot=True)\n",
    "# Factor 2 is money; factor 1 is weird - a kind of balance of food insecurity/quantity and diet quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA3rotated = varimax(pca3.loadings)\n",
    "pca3loadings = pd.DataFrame(PCA3rotated, index=indicators_mean_imputed.columns, columns = ['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "sns.heatmap(pca3loadings, annot=True)\n",
    "# Factor 1 - diet quality; Factor 2 - Income (&PPI); Factor 3 - quantity of food. Simpler separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and with PAF instead:\n",
    "loadings_views, loadings_models = show_loadings(indicators_mean_imputed)\n",
    "# loading matrices first, then score matrices\n",
    "scores_view, scores_models = show_score_matrix(indicators_mean_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat but this time without income/MAE for country as so highly correlated\n",
    "q=np.linalg.eig(indicators_mean_imputed.iloc[:,:6].corr())\n",
    "index = np.arange(len(q[0]))+1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(index, pd.Series(q[0]).sort_values(ascending=False), 'bo-')\n",
    "ax.set_title('Scree Plot')\n",
    "# take 3 factors. more decisive scree this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the other method for the scree\n",
    "pca2 = smp.PCA(indicators_mean_imputed.iloc[:,:6], method = 'nipals', missing='drop-row', ncomp=7)\n",
    "scree(pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA again\n",
    "pca2 = smp.PCA(indicators_mean_imputed.iloc[:,:6], method = 'nipals', missing='drop-row', ncomp=3)\n",
    "PCA2rotated = varimax(pca2.loadings)\n",
    "pca2loadings = pd.DataFrame(PCA2rotated, index=indicators_mean_imputed.columns[:6], columns = ['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "sns.heatmap(pca2loadings, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND WITH PAF (two and then three factors, loadings then scores):\n",
    "\n",
    "loadings_views, loadings_models = show_loadings(indicators_mean_imputed.iloc[:,:6])\n",
    "scores_views, scores_models = show_score_matrix(indicators_mean_imputed.iloc[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again PPI and income decile load onto the same factor\n",
    "# Factor 1 - Quality of diet (or lack thereof)\n",
    "# Factor 2 - Quantity of food\n",
    "# Factor 3 - Relative income\n",
    "# I think that this is more helpful\n",
    "\n",
    "# see bottom for results from other imputations, as I think they are spurious for doing factor analysis\n",
    "\n",
    "# going forward with the 3 factor model in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that these matrices behave as I expect...\n",
    "\n",
    "np.array(pca2loadings.transpose()) @ np.array(pca2loadings)\n",
    "# yes, this is approximately the identity. good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pca2loadings) @ np.array(pca2loadings).transpose() # why is that not also identity??!?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are much further from the identity!!\n",
    "np.array(loadings_views[1]) @ np.array(scores_views[1]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(loadings_views[1]).transpose() @ np.array(scores_views[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute factor scores manually; this is a useful process as I want to blank out small values for better separation of factors\n",
    "# (so easier interpretation)\n",
    "\n",
    "indicators_for_facs=normalise(indicators_mean_imputed[indicators_mean_imputed.columns[:6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculating the scores\n",
    "pca_scores = factor_scores(indicators_for_facs,pca2loadings, names=['Food Security PCA','Diet Quality PCA','Money PCA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paf_scores_array = scores_models[1].factor_scoring(method = 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paf_scores = pd.DataFrame(paf_scores_array, columns = ['Diet Quality PAF','Food Security PAF','Money PAF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd.concat([pca_scores, paf_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see how they compare to each other\n",
    "\n",
    "sns.jointplot(x='Food Security PCA', y='Food Security PAF', data = scores, alpha = .2)\n",
    "sns.jointplot(x='Diet Quality PCA', y='Diet Quality PAF', data = scores, alpha = .2)\n",
    "sns.jointplot(x='Money PCA', y='Money PAF', data = scores, alpha = .2)\n",
    "# the factor scores are very very similar, unsurprisingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(scores_views[1], annot=True, cmap='PiYG', ax=axes).figure.savefig(\"3 01 paf score matrix before dropping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(pca2loadings, annot=True, cmap='PiYG',ax=axes).figure.savefig(\"3 02 pca score matrix before dropping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next: check that the PAF scores are from multiplying the score matrix by the observations - have checked, they are.\n",
    "# recalculate with small loadings/score coefficients omtted\n",
    "# for FI missing, increase impact of HFIAS code\n",
    "#compare again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better interpretation, drop the small coefficients\n",
    "# and replace bigger ones with 1s\n",
    "\n",
    "paf_score_coeff = scores_views[1].copy()\n",
    "pca_score_coeff = pca2loadings.copy()\n",
    "for i in range(3):\n",
    "    paf_score_coeff[paf_score_coeff.columns[i]] = np.where(abs(paf_score_coeff[paf_score_coeff.columns[i]])<.3,\n",
    "                                                           0,paf_score_coeff[paf_score_coeff.columns[i]])\n",
    "    pca_score_coeff[pca_score_coeff.columns[i]] = np.where(abs(pca_score_coeff[pca_score_coeff.columns[i]])<.3,\n",
    "                                                           0,pca_score_coeff[pca_score_coeff.columns[i]])\n",
    "    paf_score_coeff[paf_score_coeff.columns[i]] = np.where((paf_score_coeff[paf_score_coeff.columns[i]])>.3,\n",
    "                                                           1,paf_score_coeff[paf_score_coeff.columns[i]])\n",
    "    pca_score_coeff[pca_score_coeff.columns[i]] = np.where((pca_score_coeff[pca_score_coeff.columns[i]])>.3,\n",
    "                                                           1,pca_score_coeff[pca_score_coeff.columns[i]])\n",
    "    paf_score_coeff[paf_score_coeff.columns[i]] = np.where((paf_score_coeff[paf_score_coeff.columns[i]])<-.3,\n",
    "                                                           -1,paf_score_coeff[paf_score_coeff.columns[i]])\n",
    "    pca_score_coeff[pca_score_coeff.columns[i]] = np.where((pca_score_coeff[pca_score_coeff.columns[i]])<-.3,\n",
    "                                                           -1,pca_score_coeff[pca_score_coeff.columns[i]])\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(paf_score_coeff, annot=True, ax=axes, cmap='PiYG').figure.savefig(\"3 03 paf score matrix after dropping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(pca_score_coeff, annot=True,ax=axes, cmap='PiYG').figure.savefig(\"3 04 pca score matrix after dropping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate factor scores manually for PCA only because losing correlations in PAF makes it pointless\n",
    "# see how they compare again\n",
    "\n",
    "pca_scores = factor_scores(indicators_for_facs,pca_score_coeff, names=['Food Security PCA','Diet Quality PCA','Money PCA'])\n",
    "paf_scores_temp = factor_scores(indicators_for_facs,paf_score_coeff, names=['Diet Quality PAF','Food Security PAF','Money PAF'])\n",
    "scores_drop_small = pd.concat ([pca_scores, paf_scores], axis=1)\n",
    "scores_temp = pd.concat ([pca_scores, paf_scores_temp], axis=1)\n",
    "sns.jointplot(x='Food Security PCA', y='Food Security PAF', data = scores_drop_small, alpha = .2)\n",
    "sns.jointplot(x='Diet Quality PCA', y='Diet Quality PAF', data = scores_drop_small, alpha = .2)\n",
    "sns.jointplot(x='Money PCA', y='Money PAF', data = scores_drop_small, alpha = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropping small coefficients has meant focusing on the points where they agree - less noise - so closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how factors are correlated in each case\n",
    "# this is the big difference\n",
    "# there is much more correlation between the factors in the PAF case\n",
    "\n",
    "# PCA, before dropping the little ones\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(scores[scores.columns[:3]].corr(), ax=axes, annot=True, cmap='BuPu', center = .2).figure.savefig('3 05 factor correlations before drop PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAF, before dropping the little ones. Much less correlation!\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(scores[scores.columns[3:]].corr(), annot=True, cmap='BuPu', ax=axes, center = .2).figure.savefig('3 06 factor correlations before drop PAF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA after dropping the little ones - little change\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(scores_drop_small[scores.columns[:3]].corr(), annot=True,ax=axes, cmap='BuPu', center = .2).figure.savefig('3 07 factor correlations after drop PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAF after dropping the little ones - the little ones are where the factors are allowed to correlate!\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(scores_temp[scores.columns[3:]].corr(), annot=True, cmap='BuPu', ax=axes, center = .2).figure.savefig('3 08 factor correlations if drop PAF')\n",
    "\n",
    "# This is tricky. Information has been lost in this part of the process.\n",
    "# But the purpose of factor analysis is to ease interpretation, and interpretation is much easier with separated factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NrofMonthsFoodInsecure had a lot of missing values.\n",
    "# For these, I'd like to amplify the effect of HFIAS code (which loads onto the same factor),\n",
    "# instead of using the imputed mean of NrofMonthsFoodInsecure.\n",
    "# Also, I want to keep the factors correlated for PAF.\n",
    "\n",
    "\n",
    "paf_score_coeff = scores_views[1]\n",
    "denominator = 0\n",
    "for i in range(len(paf_score_coeff.index)-1):\n",
    "    print(i)\n",
    "    print(paf_score_coeff.index[i+1])\n",
    "    print(paf_score_coeff.loc[paf_score_coeff.index[i+1],'Factor 2'])\n",
    "    denominator+=abs(paf_score_coeff.loc[paf_score_coeff.index[i+1],'Factor 2'])\n",
    "    print('den = '+str(denominator))\n",
    "print(paf_score_coeff.index[0])\n",
    "numerator = denominator + abs(paf_score_coeff.loc[paf_score_coeff.index[0],'Factor 2'])\n",
    "print(numerator)\n",
    "print(np.divide(numerator,denominator))\n",
    "for i in range(len(paf_score_coeff.index)-1):\n",
    "    scores_drop_small['Food Security PAF']=np.where(indicators['FI missing']==1,\n",
    "                                     indicators_for_facs[paf_score_coeff.index[i+1]]*paf_score_coeff.loc[paf_score_coeff.index[i+1],'Factor 2']*np.divide(numerator, denominator),\n",
    "                                     scores_drop_small['Food Security PAF'])\n",
    "                                                    \n",
    "scores_drop_small['Food Security PCA']=np.where(indicators['FI missing']==1,\n",
    "                                     indicators_for_facs['HFIAS_code']*(pca_score_coeff.loc['HFIAS_code','Factor 1']+pca_score_coeff.loc['NrofMonthsFoodInsecure','Factor 1']),\n",
    "                                     scores_drop_small['Food Security PCA'])\n",
    "indicators_scores = pd.concat([indicators, scores_drop_small], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at the correlations between the factor scores - little change since last time.\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(indicators_scores[['Food Security PCA', 'Diet Quality PCA', 'Money PCA']].corr(), annot=True, ax=axes, cmap='BuPu', center = .2).figure.savefig('3 09 factor correlations PCA final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_axes([.3,.1,.9,.9])\n",
    "sns.heatmap(indicators_scores[['Diet Quality PAF', 'Food Security PAF', 'Money PAF']].corr(),ax=axes, annot=True, cmap='BuPu', center = .2).figure.savefig('3 10 factor correlations PAF final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK HOW THE SCATTERS COMPARE\n",
    "\n",
    "sns.jointplot(x='Food Security PCA', y='Food Security PAF', data = indicators_scores, alpha = .2).savefig('3 11 food security PCA vs PAF scores')\n",
    "sns.jointplot(x='Diet Quality PCA', y='Diet Quality PAF', data = indicators_scores, alpha = .2).savefig('3 12 diet quality PCA vs PAF scores')\n",
    "sns.jointplot(x='Money PCA', y='Money PAF', data = scores_drop_small, alpha = .2).savefig('3 13 money PCA vs PAF scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save outfile\n",
    "indicators_scores.to_csv(\"3PCAonImputedValues_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't really know which to use - but will stick with both for now. PCA still seems easier to interpret;\n",
    "# PAF seems to give nicer distributions,\n",
    "# I'll try both.\n",
    "\n",
    "# Below - archive. Error message to prevent it from running automatically.\n",
    "sns.hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA WITH MEDIAN IMPUTATION\n",
    "indicators_success=indicators[['NrofMonthsFoodInsecure','score_PPI','HFIAS_code','score_HDDSBadSeason', 'score_HDDSGoodSeason',\n",
    "                               'Income/MAE decile']]\n",
    "indicators_success_median=indicators[['FIfromMedian','score_PPI','HFIAS_code','score_HDDSBadSeason', 'score_HDDSGoodSeason',\n",
    "                               'Income/MAE decile']]\n",
    "# indicators_success_mean=indicators[['FIfromMean','score_PPI','HFIAS_code','score_HDDSBadSeason', 'score_HDDSGoodSeason',\n",
    "           #                    'Income/MAE decile']]\n",
    "indicators_success_tree=indicators[['FI imputed pure','score_PPI','HFIAS_code','score_HDDSBadSeason', 'score_HDDSGoodSeason',\n",
    "                               'Income/MAE decile']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how the different imputations affect correlations - very little.\n",
    "fig = plt.figure(figsize = (6,30))\n",
    "axes = fig.subplots(4, gridspec_kw={\"hspace\":.6})\n",
    "descriptions = [\"Original\",\"Mean imputation\",\"Median imputation\",\"Tree imputation\"]\n",
    "indicator_sets =[indicators_success,indicators_mean_imputed[indicators_mean_imputed.columns[:6]],indicators_success_median,indicators_success_tree]\n",
    "for i in range(4):\n",
    "    sns.heatmap(indicator_sets[i].corr(method = 'spearman'),annot=True, center=1, ax=axes[i])\n",
    "    axes[i].set_title(descriptions[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually plot scree plots of the three different imputations\n",
    "fig = plt.figure(figsize = (13,3))\n",
    "axes = fig.subplots(1,3)\n",
    "for i in range (3):\n",
    "    q=np.linalg.eig(indicator_sets[i+1].corr())\n",
    "    index = np.arange(len(q[0]))+1\n",
    "    axes[i].plot(index, pd.Series(q[0]).sort_values(ascending=False), 'bo-')\n",
    "    axes[i].set_title('Scree Plot '+descriptions[i+1])\n",
    "# all suggest extraction of 3 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    pca = smp.PCA(indicator_sets[i+1], method = 'nipals', missing='drop-row', ncomp=7)\n",
    "    print(descriptions[i+1])\n",
    "    scree(pca)\n",
    "# Again all suggest 3 factor solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract factors and compare loading matrices\n",
    "fig = plt.figure(figsize = (15,3))\n",
    "axes = fig.subplots(1,3, gridspec_kw={'wspace':1})\n",
    "pca2loadings = [[],[],[]]\n",
    "for i in range(3):\n",
    "    pca2 = smp.PCA(indicator_sets[i+1], method = 'nipals', missing='drop-row', ncomp=3)\n",
    "    PCA2rotated = varimax(pca2.loadings)\n",
    "    pca2loadings[i] = pd.DataFrame(PCA2rotated, index=indicator_sets[i+1].columns, columns = ['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "    sns.heatmap(pca2loadings[i], annot=True, ax=axes[i])\n",
    "    axes[i].set_title(descriptions[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract factors and compare loading matrices\n",
    "fig = plt.figure(figsize = (12,10))\n",
    "axes = fig.subplots(2,2, gridspec_kw={'hspace':.2, 'wspace':.7})\n",
    "pca2loadings = [[],[],[],[]]\n",
    "for i in range(4):\n",
    "    pca2 = smp.PCA(indicator_sets[i], method = 'nipals', missing='drop-row', ncomp=3)\n",
    "    PCA2rotated = varimax(pca2.loadings)\n",
    "    pca2loadings[i] = pd.DataFrame(PCA2rotated, index=indicator_sets[i].columns, columns = ['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "    sns.heatmap(pca2loadings[i], annot=True, ax=axes[int(np.divide(i,2)),np.remainder(i,2)])\n",
    "    axes[int(np.divide(i,2)),np.remainder(i,2)].set_title(descriptions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What's going on; the imputation using the mean is the one which differs the most from the original.\n",
    "# the original dropped the rows where there were missing values, thus losing information.\n",
    "# the median and tree imputations attempted to fill these rows by using the available rows.\n",
    "# this creates relatioships between the data, and by assuming that these rows are the same as the others, perhaps we get no more\n",
    "#     information from them.\n",
    "\n",
    "# the mean imputation means that where a data point is missing, that point takes the neutral value - ie nothing is added or \n",
    "# subtracted for that observation of that variable. \n",
    "\n",
    "# I think that what these differences tell us is that the missing values are not missing at random, and it is\n",
    "# not prudent to impute them from these other indicators, as including the best part of these rows with the values treated as\n",
    "# missing, rather than as found from the rest of the data, yields different results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators_mean_imputed.describe()\n",
    "# all of these indicators have a natural maximum with which to work, it would be nice to have \n",
    "# meaningful maximum factor scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadings = pca2loadings[1]\n",
    "indicator_sets[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators_for_facs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(loadings, annot=True)\n",
    "# note that the negative ones are good: having low nr of months food insecure and HFIAS 0 indicates food security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for better interpretation, drop the small coefficients\n",
    "for i in range(3):\n",
    "    loadings[loadings.columns[i]] = np.where(abs(loadings[loadings.columns[i]])<.3,0,loadings[loadings.columns[i]])\n",
    "fig = sns.heatmap(loadings, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds_with_scores = pd.concat([indicators,fac_scores],axis=1)\n",
    "inds_with_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop redundant columns\n",
    "inds_with_scores.drop(['Unnamed: 0', 'Unnamed: 0.1','imputed FI','imputed FI pure'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds_with_scores.to_csv(\"3PCAonImputedValues_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just a quick check\n",
    "# factor analysis states that x = LF\n",
    "# so...\n",
    "factor_scores(inds_with_scores[['Food Security Factor','Diet Quality Factor','Money Factor']], loadings.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = normalise(indicator_sets[1])\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds_with_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds_with_scores[['NrofMonthsFoodInsecure', 'score_PPI','score_HDDSGoodSeason','score_HDDSBadSeason','HFIAS_code','Income/MAE decile',\n",
    "                  'Food Security Factor','Diet Quality Factor', 'Money Factor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
